<!DOCTYPE html>

<html lang="en-US">
<head>
<meta charset="UTF-8">
<title>Modeling of Speech-dependent Own Voice Transfer Characteristics for Hearables with an In-ear Microphone</title>
<link rel="stylesheet" href="styles.css" />
</head>

<body>

<div>
<h1>
Modeling of Speech-dependent Own Voice Transfer Characteristics for Hearables with an In-ear Microphone</h1>
<p>
Mattes Ohlenbusch, Christian Rollwage, Simon Doclo
</p>

<h2>Abstract</h2>
<p>
Many hearables contain an in-ear microphone, which may be used to capture the own voice of its user.
However, due to the hearable occluding the ear canal, the in-ear microphone mostly records body-conducted speech, typically suffering from band-limitation effects and amplification at low frequencies. 
Since the occlusion effect is determined by the ratio between the air-conducted and body-conducted components of own voice, the own voice transfer characteristics between the outer face of the hearable and the in-ear microphone depend on the speech content and the individual talker. 
In this paper, we propose a speech-dependent model of the own voice transfer characteristics based on phoneme recognition, assuming a linear time-invariant relative transfer function for each phoneme.
We consider both individual models as well as models averaged over several talkers.
Experimental results based on recordings with a prototype hearable show that the proposed speech-dependent model enables to simulate in-ear signals more accurately than a speech-independent model in terms of technical measures, especially under utterance mismatch and talker mismatch.
Additionally, simulation results show that talker-averaged models generalize better to different talkers than individual models. 
</p>
</div>

<h2>Links</h2>
<p>
Journal paper: <a href="https://doi.org/10.1051/aacus/2024032">https://doi.org/10.1051/aacus/2024032</a>
</p>
<p>
Arxiv preprint: <a href="https://arxiv.org/abs/2310.06554">https://arxiv.org/abs/2310.06554</a>
</p>
<p>
Dataset of German own voice recordings: <a href="https://doi.org/10.5281/zenodo.10844599">https://doi.org/10.5281/zenodo.10844599</a>
</p>

<h2>Spectrograms</h2>
<figure>
	<img src="specgram_AA.png" alt="Example Spectrograms">
	<figcaption>Spectrograms recorded own voice signals at outer and in-ear microphone, and simulated in-ear own voice signals.</figcaption>
</figure>

<h2>Audio Examples</h2>

<table>

<tr> 
<td>
recorded outer microphone <br>
<audio controls="controls"><source src="plot0_VP_02_0.wav" type="audio/x-wav" /></audio>
</td>
<td>
recorded in-ear microphone <br>
<audio controls="controls"><source src="plot1_VP_02_0.wav" type="audio/x-wav" /></audio>
</td>
</tr>

<tr> 
<td>
simulated in-ear <br> (speech-independent individual) <br>
<audio controls="controls"><source src="plot2_VP_02_0.wav" type="audio/x-wav" /></audio>
</td>
<td>
simulated in-ear <br> (speech-independent talker-averaged) <br>
<audio controls="controls"><source src="plot3_VP_02_0.wav" type="audio/x-wav" /></audio>
</td>
</tr>

<tr> 
<td>
simulated in-ear <br> (speech-dependent individual) <br>
<audio controls="controls"><source src="plot4_VP_02_0.wav" type="audio/x-wav" /></audio>
</td>
<td>
simulated in-ear <br> (speech-dependent talker-averaged) <br>
<audio controls="controls"><source src="plot5_VP_02_0.wav" type="audio/x-wav" /></audio>
</td>
</tr>
<table>


</body>
</html>
